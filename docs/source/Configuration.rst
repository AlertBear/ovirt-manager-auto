Configuration
--------------
Open conf/settings.conf file. This is an example of your main configuration file, it contains the following sections: 

[RUN] section
=============
* **engine** - if you have several engines supported in project and based on REST – sdk, cli, etc. - you can switch between the engines by setting the proper value of this parameter. By default the ‘rest’ engine is used.
* **tests_file** - name of the file which contains your test scenario. If this file is located in the conf/ folder, you only need to specify its relative name; otherwise specify the absolute path to the file.  You can list here several files that should be run with comma separation. The input file can be in XML format, ODS Spreadsheet or a python script.
* **tests_sheets** -  the sheets from your test scenario file which should be run, relevant only for ODS files.
* **in_parallel** - optional parameter for parallel runs. List of the sheets or xml files which should be run in parallel. You can run different sheets/xml files or the same sheet/xml file with different configurations files in parallel. See the example below. 
* **parallel_configs** - optional parameter for parallel runs. Paths to configuration files can be set here (separated by commas) for each of the sheet or xml file running in parallel. If this parameter is omitted - default configuration file of test is taken. 
* **debug** - if the test should be run in debug mode or no, if this parameter set to 'no', only INFO and ERROR messages will be printed to the log file 
* **data_struct_mod** - path to data structures file generated by generateDS (see the previous section how to create it)
* **api_xsd** - path to xml schema documents file
* **media_type** - application/xml for xml format, application/json for json format

Example for RUN section simple configuration::

    [RUN]
    engine = rest
    tests_file = /tmp/rest.xml
    data_struct_mod = data_struct.data_structures
    api_xsd = /data_struct/api.xsd
    debug = yes
    media_type = application/xml

Example for the same xml file running in parallel with different configuration files::

    [RUN]
    tests_file = /tmp/test.xml, /tmp/test.xml
    in_parallel=/tmp/test.xml, /tmp/test.xml # same or different xml files can be set here
    parallel_configs = conf/settings.conf, conf/settings2.conf # paths to config files, if omitted - default config is set

[REST_CONNECTION] section
=========================
This section defines parameters for connecting to REST API application:

* **scheme** - 'http' or 'https' 
* **host** - your REST client machine 
* **port** - port which is used by REST APIs
* **entry_point** - REST APIs main url suffix (for an example for <host>:<port>/api/ entry_point=api)
* **user** - user name to connect to REST APIs client
* **user_domain** - user domain

Example for the secured http connection::

    [REST_CONNECTION]
    scheme = https
    host = host.redhat.com
    port = 443
    user = admin
    password = 123456
    entry_point = api
    user_domain = qa.lab.redhat.com

[PARAMETERS] section
====================
This section defines global parameters used by the test. They are optional and can be removed or extended depending on test scenario definition (in your input test scenario file). Any of these parameters could be a single value or a list or values (separated by commas). All these parameters can be accessed from test scenario file by different place holders.

Customized sections for parallel run
=====================================
You can run the same test file in parallel with different parameters from one configuration file. For this purpose you should define new configuration sections with required parameters for each of the parallel threads. Here is an example::

    [RUN]
    tests_file = /tmp/test.xml,/tmp/test.xml
    in_parallel=/tmp/test.xml,/tmp/test.xml
    parallel_sections = test1, test2
    ...

    [TEST1]
    name=test1

    [TEST2]
    name=test2


[REPORT] section
==================
This section defines parameters required for tests results reporting. 

* **has_sub_tests** - 'yes' if each test should be reported as a sub-test of its group/testing module, 'no' if each test should be reported as independent 
* **add_report_nodes** - list of nodes names if additional nodes besides the default ones (defined in input xml nodes) should be added to report, 'no' otherwise. Example of added nodes format string: add_report_nodes=version, name.

.. note::

    If you want to add additional nodes to the report  make sure that the function you use in your test scenario returns these properties in its output dictionary